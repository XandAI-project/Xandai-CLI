#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
XandAI - Main CLI Entry Point
Production-ready CLI assistant with multi-provider support
Enhanced with OS-aware utilities and intelligent prompts
"""

import argparse
import json
import os
import platform
import sys
from pathlib import Path
from typing import Optional

# Ensure UTF-8 encoding for Windows compatibility
if os.name == "nt":  # Windows
    import codecs
    import locale

    # Set UTF-8 encoding for stdout/stderr to avoid UnicodeEncodeError
    sys.stdout = codecs.getwriter("utf-8")(sys.stdout.detach())
    sys.stderr = codecs.getwriter("utf-8")(sys.stderr.detach())

    # Set default encoding for subprocess operations
    os.environ["PYTHONIOENCODING"] = "utf-8"

from xandai.chat import ChatREPL
from xandai.history import HistoryManager
from xandai.integrations.base_provider import LLMProvider
from xandai.integrations.provider_factory import LLMProviderFactory
from xandai.utils.os_utils import OSUtils
from xandai.utils.prompt_manager import PromptManager


def create_parser() -> argparse.ArgumentParser:
    """Create and configure argument parser with OS-aware debug options"""
    parser = argparse.ArgumentParser(
        prog="xandai",
        description="XandAI - Multi-Provider AI Terminal Assistant with Interactive Code Execution",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=f"""
üöÄ Multi-Provider Support:
  ‚Ä¢ Ollama (local LLM server)
  ‚Ä¢ LM Studio (OpenAI-compatible API)
  ‚Ä¢ Auto-detection of available providers

üìã Interactive Features:
  ‚Ä¢ Smart code detection and execution prompts
  ‚Ä¢ Toggle interactive mode with /interactive command
  ‚Ä¢ Cross-platform terminal command integration
  ‚Ä¢ Real-time conversation with context tracking

Examples:
  xandai                                    # Start with auto-detected provider
  xandai --provider ollama                  # Use Ollama specifically
  xandai --provider lm_studio               # Use LM Studio
  xandai --auto-detect                      # Auto-detect best provider
  xandai --endpoint http://192.168.1.10:11434  # Custom Ollama server
  xandai --debug --platform-info           # Debug mode with platform info

üéØ Interactive Commands (available in REPL):
  /help               - Show available commands
  /interactive        - Toggle code execution prompts
  /status             - Show provider and model status
  /task <description> - [DEPRECIADO] Structured project planning mode
  /debug              - Toggle debug information
  /exit               - Exit XandAI

Platform: {OSUtils.get_platform().upper()} ({platform.system()} {platform.release()})
        """,
    )

    # Provider and connection options
    parser.add_argument(
        "--provider",
        metavar="PROVIDER",
        default="ollama",
        choices=["ollama", "lm_studio"],
        help="LLM provider to use (default: ollama) - 'ollama' for local Ollama server, 'lm_studio' for LM Studio OpenAI-compatible API",
    )

    parser.add_argument(
        "--endpoint",
        metavar="URL",
        help="Provider server endpoint (auto-detected if not specified)",
    )

    parser.add_argument(
        "--model",
        metavar="MODEL",
        help="Model to use (will prompt to select if not specified)",
    )

    # Debug and platform options
    parser.add_argument("--verbose", action="store_true", help="Enable verbose logging")

    parser.add_argument(
        "--debug",
        action="store_true",
        help="Enable debug mode with detailed OS information",
    )

    parser.add_argument(
        "--platform-info",
        action="store_true",
        help="Show detailed platform information at startup",
    )

    parser.add_argument(
        "--show-commands",
        action="store_true",
        help="Show available OS-specific commands and exit",
    )

    parser.add_argument(
        "--auto-detect",
        action="store_true",
        help="Auto-detect best available provider (scans for Ollama and LM Studio servers)",
    )

    parser.add_argument(
        "--test-commands",
        action="store_true",
        help="Test OS-specific commands with sample files and exit",
    )

    parser.add_argument(
        "--system-prompt",
        choices=["chat", "task", "command"],
        help="Show system prompt for specified mode and exit",
    )

    parser.add_argument(
        "--version", action="version", version="XandAI 2.1.5 - Multi-Provider Edition"
    )

    return parser


def show_platform_info():
    """Show detailed platform information"""
    print("üñ•Ô∏è  Platform Information")
    print("=" * 50)
    print(f"Operating System: {OSUtils.get_platform().title()}")
    print(f"Platform Name: {platform.system()}")
    print(f"Platform Release: {platform.release()}")
    print(f"Platform Version: {platform.version()}")
    print(f"Machine Type: {platform.machine()}")
    print(f"Processor: {platform.processor()}")
    print(f"Architecture: {platform.architecture()[0]}")
    print(f"Python Version: {platform.python_version()}")
    print(f"Is Windows: {OSUtils.is_windows()}")
    print(f"Is Unix-like: {OSUtils.is_unix_like()}")
    print()


def show_os_commands():
    """Show available OS-specific commands"""
    commands = OSUtils.get_available_commands()

    print("üìã Available OS-Specific Commands")
    print("=" * 50)
    print(f"Platform: {OSUtils.get_platform().upper()}")
    print()

    for cmd_type, cmd_template in commands.items():
        print(f"‚Ä¢ {cmd_type.replace('_', ' ').title()}: {cmd_template}")

    print()
    print("Usage Examples:")
    print(f"‚Ä¢ Read file: {OSUtils.get_file_read_command('example.txt')}")
    print(f"‚Ä¢ List directory: {OSUtils.get_directory_list_command('.')}")
    print(f"‚Ä¢ Search pattern: {OSUtils.get_file_search_command('TODO', 'src/')}")
    print()


def test_os_commands():
    """Test OS-specific commands with sample scenarios"""
    print("üîß Testing OS-Specific Commands")
    print("=" * 50)

    # Test file reading commands
    test_files = ["README.md", "setup.py", "requirements.txt"]
    existing_files = [f for f in test_files if Path(f).exists()]

    if existing_files:
        test_file = existing_files[0]
        print(f"Testing with existing file: {test_file}")
        print()

        print("Commands that would be generated:")
        print(f"‚Ä¢ Read entire file: {OSUtils.get_file_read_command(test_file)}")
        print(f"‚Ä¢ First 5 lines: {OSUtils.get_file_head_command(test_file, 5)}")
        print(f"‚Ä¢ Last 5 lines: {OSUtils.get_file_tail_command(test_file, 5)}")
        print(f"‚Ä¢ Search 'import': {OSUtils.get_file_search_command('import', test_file)}")
        print()

        # Test directory commands
        print(f"‚Ä¢ List current dir: {OSUtils.get_directory_list_command('.')}")
        if OSUtils.is_windows():
            print("‚Ä¢ PowerShell commands available for advanced operations")
        else:
            print("‚Ä¢ Unix commands available with powerful options")
    else:
        print("No test files found in current directory")

    print()
    print("Debug output test:")
    OSUtils.debug_print("This is a test debug message", True)
    OSUtils.debug_print("This debug message won't show", False)
    print()


def show_system_prompt(mode: str):
    """Show system prompt for specified mode"""
    print(f"ü§ñ System Prompt for {mode.upper()} Mode")
    print("=" * 50)

    if mode == "chat":
        prompt = PromptManager.get_chat_system_prompt()
    elif mode == "task":
        prompt = PromptManager.get_task_system_prompt_full_project()
    elif mode == "command":
        prompt = PromptManager.get_command_generation_prompt()
    else:
        print(f"Unknown mode: {mode}")
        return

    print(prompt)
    print()
    print("=" * 50)
    print(f"Prompt length: {len(prompt)} characters")
    print()


def main():
    """Main CLI entry point with OS-aware debugging and enhanced functionality"""
    parser = create_parser()
    args = parser.parse_args()

    # Handle debug/info commands that exit immediately
    if args.show_commands:
        show_os_commands()
        sys.exit(0)

    if args.test_commands:
        test_os_commands()
        sys.exit(0)

    if args.system_prompt:
        show_system_prompt(args.system_prompt)
        sys.exit(0)

    try:
        # Show platform info if requested
        if args.platform_info or args.debug:
            show_platform_info()

        # Debug initialization
        if args.debug:
            OSUtils.debug_print(f"Debug mode enabled on {OSUtils.get_platform()}", True)
            OSUtils.debug_print(
                f"Available OS commands: {list(OSUtils.get_available_commands().keys())}",
                True,
            )
            OSUtils.debug_print(
                f"Prompt manager initialized with {len(PromptManager.__dict__)} methods",
                True,
            )

        # Initialize LLM Provider
        print("üîå Initializing LLM provider...")

        if args.auto_detect:
            if args.debug:
                OSUtils.debug_print("Auto-detecting best available provider", True)
            print("üîç Auto-detecting best available provider...")
            llm_provider = LLMProviderFactory.create_auto_detect()
        else:
            if args.debug:
                OSUtils.debug_print(f"Creating {args.provider} provider", True)

            config_options = {}
            if args.endpoint:
                config_options["base_url"] = args.endpoint
            if args.model:
                config_options["model"] = args.model

            llm_provider = LLMProviderFactory.create_provider(
                provider_type=args.provider, **config_options
            )

        # Check connection
        if not llm_provider.is_connected():
            provider_name = llm_provider.get_provider_type().value.title()
            endpoint = llm_provider.get_base_url()

            print(f"‚ùå Could not connect to {provider_name} at {endpoint}")
            print(f"Please ensure {provider_name} is running and accessible.")

            # Provider-specific help
            if llm_provider.get_provider_type().value == "ollama":
                if OSUtils.is_windows():
                    print("Windows: Try running 'ollama serve' in a separate PowerShell window")
                else:
                    print("Unix-like: Try running 'ollama serve' in a separate terminal")
            elif llm_provider.get_provider_type().value == "lm_studio":
                print("Make sure LM Studio is running with a model loaded")
                print("Check the 'Server' tab in LM Studio and ensure it's started")

            if args.debug:
                OSUtils.debug_print(
                    f"Connection failed - check if {provider_name} service is running",
                    True,
                )
                OSUtils.debug_print(f"Endpoint attempted: {endpoint}", True)

            sys.exit(1)

        provider_name = llm_provider.get_provider_type().value.title()
        if args.debug:
            OSUtils.debug_print(f"{provider_name} connection successful", True)
        print(f"‚úÖ Connected to {provider_name} successfully!")

        # Get available models
        models = llm_provider.list_models()
        if not models:
            provider_name = llm_provider.get_provider_type().value.title()
            print(f"‚ùå No models found on {provider_name} server.")

            if llm_provider.get_provider_type().value == "ollama":
                if OSUtils.is_windows():
                    print("Try: ollama pull llama3.2 (in PowerShell)")
                else:
                    print("Try: ollama pull llama3.2 (in terminal)")
            elif llm_provider.get_provider_type().value == "lm_studio":
                print("Load a model in LM Studio first")

            sys.exit(1)

        if args.debug:
            OSUtils.debug_print(f"Found {len(models)} models: {models}", True)

        # Handle model selection
        current_model = llm_provider.get_current_model()
        if args.model:
            # User specified a model
            if args.model in models:
                llm_provider.set_model(args.model)
                print(f"üì¶ Using model: {args.model}")
                if args.debug:
                    OSUtils.debug_print(f"Model set to: {args.model}", True)
            else:
                print(f"‚ùå Model '{args.model}' not found.")
                print(f"Available models: {', '.join(models)}")
                sys.exit(1)
        elif not current_model:
            # No model specified - always show selection if multiple models
            if len(models) == 1:
                llm_provider.set_model(models[0])
                print(f"üì¶ Auto-selected model: {models[0]} (only model available)")
                if args.debug:
                    OSUtils.debug_print(f"Auto-selected single model: {models[0]}", True)
            else:
                # Always show interactive selection when multiple models available
                print()
                print(
                    f"\033[1;36müì¶ Available models on {llm_provider.get_provider_type().value.title()} server:\033[0m"
                )
                print("\033[1;35m" + "‚ïî" + "‚ïê" * 70 + "‚ïó\033[0m")

                for i, model in enumerate(models, 1):
                    # Extract model name parts for better formatting
                    if "/" in model:
                        parts = model.split("/")
                        org = parts[0] if len(parts) > 0 else ""
                        name = "/".join(parts[1:]) if len(parts) > 1 else model
                        formatted = f"\033[90m{org}/\033[0m\033[1;33m{name}\033[0m"
                    else:
                        formatted = f"\033[1;33m{model}\033[0m"

                    print(f"\033[1;35m‚ïë\033[0m \033[1;32m{i:2d}.\033[0m {formatted}")

                print("\033[1;35m" + "‚ïö" + "‚ïê" * 70 + "‚ïù\033[0m")
                print()

                while True:
                    try:
                        choice = input(
                            f"\033[1;36m‚Üí Select model (1-{len(models)}):\033[0m "
                        ).strip()
                        if choice.isdigit():
                            idx = int(choice) - 1
                            if 0 <= idx < len(models):
                                selected_model = models[idx]
                                llm_provider.set_model(selected_model)
                                print(
                                    f"\033[1;32m‚úì Using model:\033[0m \033[1;33m{selected_model}\033[0m"
                                )
                                if args.debug:
                                    OSUtils.debug_print(
                                        f"User selected model: {selected_model}", True
                                    )
                                break
                        print("\033[1;31m‚úó Invalid selection. Please try again.\033[0m")
                    except (KeyboardInterrupt, EOFError):
                        print("\nüëã Goodbye!")
                        sys.exit(0)
        else:
            # Model was auto-selected or already configured
            print(f"üì¶ Using model: {current_model}")
            if args.debug:
                OSUtils.debug_print(f"Using configured model: {current_model}", True)

        # Initialize history manager
        history_manager = HistoryManager()
        if args.debug:
            OSUtils.debug_print("History manager initialized", True)

        # Show ASCII title and startup info
        provider_name = llm_provider.get_provider_type().value.title()
        current_model = llm_provider.get_current_model() or "None"

        # Beautiful ASCII art logo with sunset gradient (red ‚Üí orange ‚Üí yellow)
        print()
        print("\033[38;5;196m ‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ñà‚ïó   ‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó   ‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ïó\033[0m")
        print("\033[38;5;202m ‚ïö‚ñà‚ñà‚ïó‚ñà‚ñà‚ïî‚ïù ‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ïë ‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó ‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó ‚ñà‚ñà‚ïë\033[0m")
        print("\033[38;5;208m  ‚ïö‚ñà‚ñà‚ñà‚ïî‚ïù  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïë ‚ñà‚ñà‚ïî‚ñà‚ñà‚ïó ‚ñà‚ñà‚ïë ‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïë ‚ñà‚ñà‚ïë\033[0m")
        print("\033[38;5;214m  ‚ñà‚ñà‚ïî‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïë ‚ñà‚ñà‚ïë‚ïö‚ñà‚ñà‚ïó‚ñà‚ñà‚ïë ‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë ‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïë ‚ñà‚ñà‚ïë\033[0m")
        print("\033[38;5;220m ‚ñà‚ñà‚ïî‚ïù ‚ñà‚ñà‚ïó ‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë ‚ñà‚ñà‚ïë ‚ïö‚ñà‚ñà‚ñà‚ñà‚ïë ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù ‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë ‚ñà‚ñà‚ïë\033[0m")
        print("\033[38;5;226m ‚ïö‚ïê‚ïù  ‚ïö‚ïê‚ïù ‚ïö‚ïê‚ïù  ‚ïö‚ïê‚ïù ‚ïö‚ïê‚ïù  ‚ïö‚ïê‚ïê‚ïê‚ïù ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù  ‚ïö‚ïê‚ïù  ‚ïö‚ïê‚ïù ‚ïö‚ïê‚ïù\033[0m")
        print()
        print("\033[38;5;214m  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ïó      ‚ñà‚ñà‚ïó\033[0m")
        print("\033[38;5;220m ‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù ‚ñà‚ñà‚ïë      ‚ñà‚ñà‚ïë\033[0m")
        print("\033[38;5;226m ‚ñà‚ñà‚ïë      ‚ñà‚ñà‚ïë      ‚ñà‚ñà‚ïë\033[0m")
        print("\033[38;5;228m ‚ñà‚ñà‚ïë      ‚ñà‚ñà‚ïë      ‚ñà‚ñà‚ïë\033[0m")
        print("\033[38;5;230m ‚ïö‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ïë\033[0m")
        print("\033[38;5;231m  ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù ‚ïö‚ïê‚ïù\033[0m")
        print()

        # Provider and Model info with better formatting
        box_width = max(len(f"Provider: {provider_name}"), len(f"Model: {current_model}")) + 6
        border_line = "‚ïê" * box_width

        print("\033[1;35m‚ïî" + border_line + "‚ïó\033[0m")
        provider_line = f"Provider: \033[1;32m{provider_name}\033[1;35m"
        provider_padding = box_width - len(f"Provider: {provider_name}")
        print("\033[1;35m‚ïë " + provider_line + " " * provider_padding + " ‚ïë\033[0m")

        model_line = f"Model:    \033[1;33m{current_model}\033[1;35m"
        model_padding = box_width - len(f"Model:    {current_model}")
        print("\033[1;35m‚ïë " + model_line + " " * model_padding + " ‚ïë\033[0m")
        print("\033[1;35m‚ïö" + border_line + "‚ïù\033[0m")
        print()

        print("\033[1;32müöÄ Starting XandAI REPL...\033[0m")
        print("Type '\033[1;36mhelp\033[0m' for commands or start chatting!")
        print(
            "\033[1;33m‚ö†Ô∏è  Note: /task command is deprecated. Use natural conversation instead.\033[0m"
        )

        # OS-specific command hints
        if OSUtils.is_windows():
            print("Windows commands supported: type, dir, findstr, powershell, etc.")
        else:
            print("Unix commands supported: cat, ls, grep, head, tail, etc.")

        if args.debug:
            print(
                f"üîß DEBUG MODE: Platform={OSUtils.get_platform().upper()}, Verbose={args.verbose}"
            )

        print("-" * 50)

        # Enhanced REPL with LLM Provider
        repl = ChatREPL(llm_provider, history_manager, verbose=args.verbose or args.debug)

        if args.debug:
            OSUtils.debug_print("Starting REPL with enhanced configuration", True)

        repl.run()

    except KeyboardInterrupt:
        if args.debug:
            OSUtils.debug_print("Received KeyboardInterrupt", True)
        print("üëã Goodbye!")
        sys.exit(0)
    except Exception as e:
        print(f"‚ùå Fatal error: {e}")
        if args.verbose or args.debug:
            import traceback

            traceback.print_exc()
            if args.debug:
                OSUtils.debug_print(f"Fatal error details: {str(e)}", True)
        sys.exit(1)


if __name__ == "__main__":
    main()
